{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUT for CREATING TFRECORDS\n",
    "\n",
    "[medium-post](https://towardsdatascience.com/a-practical-guide-to-tfrecords-584536bc786c)\n",
    "\n",
    "- [done] create tfrecords for images\n",
    "- [done] sharding \n",
    "- [ ] create tfrecords for text\n",
    "- [ ] create tfrecords for audio "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TFRecords Converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(data):\n",
    "    \"\"\"returns a bytes_list from a string / byte\"\"\"\n",
    "    if isinstance(data , type(tf.constant(0))): # if value is tensorflow type\n",
    "        data = data.numpy() # get value of tensor\n",
    "    bytes_list = tf.train.BytesList(value=[data]) # create bytes list\n",
    "    feature = tf.train.Feature(bytes_list=bytes_list) # create feature from bytes list\n",
    "    return feature\n",
    "\n",
    "\n",
    "def _float_feature(data):\n",
    "    \"\"\"returns a float_list from a float / double\"\"\"\n",
    "    float_list = tf.train.FloatList(value=[data]) # create float list\n",
    "    feature = tf.train.Feature(float_list=float_list) # create feature from float list\n",
    "    return feature\n",
    "\n",
    "def _int64_feature(data):\n",
    "    \"\"\"returns a int64_list from a bool / enum / int / uint\"\"\"\n",
    "    int64_list = tf.train.Int64List(value=[data]) # create int64 list\n",
    "    feature = tf.train.Feature(int64_list=int64_list) # create feature from int64 list\n",
    "    return feature\n",
    "\n",
    "def serialize_array(array):\n",
    "    array = tf.io.serialize_tensor(array) # serialize tensor\n",
    "    return array\n",
    "\n",
    "def deserialize_array(array):\n",
    "    array = tf.io.parse_tensor(array, out_type=tf.float32) # parse tensor\n",
    "    return array\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## Create a random image\n",
    "# image_small_shape = (250, 250, 3)\n",
    "# num_small_images = 100\n",
    "\n",
    "# images_small =np.random.randint(\n",
    "#     low=0, high = 255, size = (num_small_images, *image_small_shape), dtype=np.int16\n",
    "# )\n",
    "\n",
    "# print(images_small.shape)\n",
    "# #plt.imshow(imgaes_small[0])\n",
    "\n",
    "# ## Create a label \n",
    "# labels_small = np.random.randint(\n",
    "#     low =0 , high = 5, size = (num_small_images, 1) \n",
    "# )\n",
    "# print(labels_small.shape)\n",
    "# labels_small[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_images(images, labels):\n",
    "    examples = []\n",
    "    for idx in range(len(images)):\n",
    "        curr_image = images[idx]\n",
    "        curr_label = labels[idx]\n",
    "\n",
    "        data = {\n",
    "            'height': _int64_feature(curr_image.shape[0]),\n",
    "            'width': _int64_feature(curr_image.shape[1]),\n",
    "            'depth': _int64_feature(curr_image.shape[2]),\n",
    "            'label': _int64_feature(curr_label[0]),\n",
    "            'image_raw': _bytes_feature(serialize_array(curr_image))\n",
    "        }\n",
    "        feat = tf.train.Features(feature=data)\n",
    "        out = tf.train.Example(features=feat)\n",
    "        examples.append(out.SerializeToString())\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def write_images_to_tfr(images, labels, filename: str = \"images\"):\n",
    "    filename = filename + \".tfrecords\"  # add extension\n",
    "    writer = tf.io.TFRecordWriter(filename)  # create writer\n",
    "\n",
    "    examples = parse_images(images, labels)\n",
    "    count = len(examples)  # keep track of how many images we write\n",
    "\n",
    "    for example in examples:\n",
    "        writer.write(example)\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Wrote {count} images to {filename}\")\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfr_element(element):\n",
    "    data = {\n",
    "        'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image_raw': tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    \n",
    "    content =tf.io.parse_single_example(element, data) # parse single example\n",
    "    \n",
    "    heigh = content['height'] # get height\n",
    "    width = content['width'] # get width\n",
    "    label = content['label'] # get label\n",
    "    depth = content['depth'] # get depth\n",
    "    raw_img = content['image_raw'] # get image raw bytes\n",
    "    \n",
    "    \n",
    "    feature = tf.io.parse_tensor(raw_img, out_type=tf.int16) # parse image raw bytes\n",
    "    feature = tf.reshape(feature, (heigh, width, depth)) # reshape image\n",
    "    return feature, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_single_image(image, label):\n",
    "  \n",
    "  #define the dictionary -- the structure -- of our single example\n",
    "  data = {\n",
    "        'height' : _int64_feature(image.shape[0]),\n",
    "        'width' : _int64_feature(image.shape[1]),\n",
    "        'depth' : _int64_feature(image.shape[2]),\n",
    "        'raw_image' : _bytes_feature(serialize_array(image)),\n",
    "        'label' : _int64_feature(label[0])\n",
    "    }\n",
    "  #create an Example, wrapping the single features\n",
    "  out = tf.train.Example(features=tf.train.Features(feature=data))\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_images_to_tfr(images_small, labels_small, filename=\"images\")\n",
    "# data_set = tf.data.TFRecordDataset(\"images.tfrecords\") # create dataset\n",
    "# data_set = data_set.map(parse_tfr_element) # parse dataset\n",
    "# data_set.batch(5) # batch dataset\n",
    "# for sample in data_set.take(5):\n",
    "#     image = sample[0].numpy()\n",
    "#     label = sample[1].numpy()\n",
    "#     print(f\"Image shape: {image.shape}\")\n",
    "#     print(f\"Label: {label}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHARDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_image_size = (500, 500, 3)\n",
    "num_big_images = 500\n",
    "\n",
    "large_images = np.random.randint(\n",
    "    low = 0, high = 255, size = (num_big_images, *big_image_size), dtype=np.int16\n",
    ")\n",
    "\n",
    "large_labels = np.random.randint(\n",
    "    low = 0, high = 5, size = (num_big_images, 1)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = (len(large_images) // 10) + 1\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_images_to_tfr_long(images, labels, filename:str = 'Large_images', max_files:int = 10, out_dir:str= \"large_TFR\"):\n",
    "    if os.path.exists(out_dir) == False:\n",
    "        os.mkdir(out_dir)\n",
    "    splits = (len(images) // max_files) + 1 # get number of splits\n",
    "    \n",
    "    if len(images) % max_files == 0:\n",
    "        splits -= 1\n",
    "        \n",
    "    print(f\"Writing {splits} files\")\n",
    "    file_count = 0\n",
    "    \n",
    "    for idx in tqdm(range(splits)):\n",
    "        current_shard_name = f\"{out_dir}/{filename}_{idx}.tfrecords\"\n",
    "        \n",
    "        writer = tf.io.TFRecordWriter(current_shard_name)\n",
    "        current_shard_count =0\n",
    "        \n",
    "        while current_shard_count < max_files: # while we have not written max files\n",
    "            # get current index\n",
    "            index = idx * max_files + current_shard_count\n",
    "            if index == len(images):\n",
    "                break\n",
    "            \n",
    "            curr_image = images[index]\n",
    "            curr_label = labels[index]\n",
    "            \n",
    "            \n",
    "            out = parse_single_image(curr_image, curr_label)\n",
    "            writer.write(out.SerializeToString())\n",
    "            current_shard_count += 1\n",
    "            file_count += 1\n",
    "            \n",
    "        writer.close()\n",
    "    print(f\"Wrote {file_count} images to {out_dir}/{filename}_*.tfrecords\")\n",
    "    return file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 17 files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b320b1a0e746cbab4a78787d32ab95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 500 images to large_TFR/Large_images_*.tfrecords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_images_to_tfr_long(large_images, large_labels, max_files=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
