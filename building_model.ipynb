{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "#* autoreload to reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#* import custom modules\n",
    "from process import *\n",
    "for gpu in tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "#* mixed precision to speed up training\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_global_policy(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (120, 120)\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = prepare_images(\"aug_data/train/images/*.jpg\", size=size)\n",
    "test_images = prepare_images(\"aug_data/test/images/*.jpg\", size=size)\n",
    "val_images = prepare_images(\"aug_data/val/images/*.jpg\", size=size)\n",
    "\n",
    "train_labels = prepare_labels(\"aug_data/train/labels/*.json\")\n",
    "test_labels = prepare_labels(\"aug_data/test/labels/*.json\")\n",
    "val_labels = prepare_labels(\"aug_data/val/labels/*.json\")\n",
    "\n",
    "train = combine(train_images, train_labels, batch_size=batch_size)\n",
    "test = combine(test_images, test_labels)\n",
    "val = combine(val_images, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(size=size):\n",
    "    input_layer = Input(shape=(size[0],size[1], 3))\n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "\n",
    "    #* 1 for classification\n",
    "    f1 = tf.keras.layers.GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(1024, activation='relu')(f1)\n",
    "    class2 = Dense(1, activation='sigmoid')(class1)\n",
    "    # class2 = tf.cast(class2, tf.float16)\n",
    "\n",
    "    #* 4 for bounding box\n",
    "    f2 = tf.keras.layers.GlobalAveragePooling2D()(vgg)\n",
    "    reggress1 = Dense(1024, activation='relu')(f2)\n",
    "    reggress2 = Dense(4, activation='sigmoid')(reggress1)\n",
    "    #reggress2 = tf.cast(reggress2, tf.float16)\n",
    "\n",
    "    face_tracker = Model(inputs = input_layer, outputs = [class2, reggress2])\n",
    "\n",
    "    return face_tracker\n",
    "\n",
    "face_tracker = build_model()\n",
    "#face_tracker.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train)\n",
    "lr_decay = (1./0.75 - 1) / (batches_per_epoch)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YOLO](https://stats.stackexchange.com/questions/319243/object-detection-loss-function-yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, y_pred):\n",
    "    # if len(y_true.shape) > 2:\n",
    "    #     y_true = tf.squeeze(y_true, axis=0)\n",
    "    #     #real_coords.shape.as_list() == pred_coords.shape.as_list()\n",
    "    # try:\n",
    "    #     delta_coord = tf.reduce_sum(tf.square(y_true[:, :2] - y_pred[:, :2]))\n",
    "    # except :\n",
    "    #     y_true = tf.squeeze(y_true, axis=0)\n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:, :2] - y_pred[:, :2]))\n",
    "    \n",
    "    try :\n",
    "        h_true = y_true[:, 3]  - y_true[:, 1]\n",
    "        w_true = y_true[:, 2]  - y_true[:, 0]\n",
    "        \n",
    "        h_pred = y_pred[:, 3]  - y_pred[:, 1]\n",
    "        w_pred = y_pred[:, 2]  - y_pred[:, 0]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(y_true)\n",
    "        print(y_pred)\n",
    "        raise e\n",
    "    \n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true - h_pred))\n",
    "    # delta_size = tf.reduce_sum(tf.square(tf.sqrt(w_true) - tf.sqrt(w_pred)) + tf.square(tf.sqrt(h_true) - tf.sqrt(h_pred)))    \n",
    "    \n",
    "    return delta_coord + 0.5*delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "regression_loss = localization_loss\n",
    "\n",
    "def total_loss (y_true, y_pred):\n",
    "    class_loss = tf.keras.losses.BinaryCrossentropy()(y_true[0], y_pred[0])\n",
    "    regression_loss = localization_loss(y_true[1], y_pred[1])\n",
    "    return class_loss + regression_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking = False\n",
    "if checking:\n",
    "    train_iter = train.as_numpy_iterator()\n",
    "    real_coords_list = []\n",
    "    pred_coords_list = []\n",
    "    class_loss_hist = []\n",
    "    regression_loss_hist = []\n",
    "    class_shape_tracker = []\n",
    "    coords_shape_tracker = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        data_sample = train_iter.next()\n",
    "        \n",
    "        img = data_sample[0]\n",
    "        labels = data_sample[1]\n",
    "        \n",
    "        real_class = labels[0]\n",
    "        real_coords = labels[1]\n",
    "        \n",
    "        #* model \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_class, pred_coords = face_tracker(img, training=False)\n",
    "            try :\n",
    "                # if len(real_coords.shape) > 2:\n",
    "                #     real_coords = tf.squeeze(real_coords, axis=0)\n",
    "                #     real_coords.shape.as_list() == pred_coords.shape.as_list()\n",
    "                # class_loss_value = class_loss(real_class, pred_class)\n",
    "                # regression_loss_value = regression_loss(real_coords, pred_coords)\n",
    "                # class_loss_hist.append(class_loss_value)\n",
    "                # regression_loss_hist.append(regression_loss_value)\n",
    "                class_shape_tracker.append(len(real_class.shape))\n",
    "                coords_shape_tracker.append(len(real_coords.shape))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(real_class)\n",
    "                print(pred_class)\n",
    "                print(real_coords)\n",
    "                print(pred_coords)\n",
    "                raise e\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model):\n",
    "    def __init__(self, model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = model\n",
    "        self.coord_track = []\n",
    "        \n",
    "    def compile(self, optimizer, class_loss, regression_loss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        self.class_loss = class_loss\n",
    "        self.regression_loss = regression_loss\n",
    "        \n",
    "    def train_step(self, batch, **kwargs):\n",
    "        x, y = batch\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            #* predict\n",
    "            classes, coords = self.model(x, training=True)\n",
    "            #self.coord_track.append(coords)\n",
    "            #* calculate loss\n",
    "            batch_class_loss = self.class_loss(y[0], classes)\n",
    "            batch_regression_loss = self.regression_loss(y[1], coords)\n",
    "            \n",
    "            #* total loss\n",
    "            total_loss = 2*batch_regression_loss + batch_class_loss\n",
    "            \n",
    "            #* get gradients\n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "            \n",
    "        #* update weights\n",
    "        self.optimizer.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        return {'loss': total_loss, 'class_loss': batch_class_loss, 'regression_loss': batch_regression_loss}\n",
    "    \n",
    "    def test_step(self, batch, **kwargs):\n",
    "        x, y = batch\n",
    "        \n",
    "        classes, coords = self.model(x, training=False)\n",
    "        batch_class_loss = self.class_loss(y[0], classes)\n",
    "        batch_regression_loss = self.regression_loss(y[1], coords)\n",
    "        \n",
    "        total_loss = batch_regression_loss +  batch_class_loss\n",
    "        return {'loss': total_loss, 'class_loss': batch_class_loss, 'regression_loss': batch_regression_loss}\n",
    "        \n",
    "    def call(self, x, **kwargs):\n",
    "        return self.model(x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_tracker = build_model()\n",
    "model = FaceTracker(face_tracker)\n",
    "model.compile(optimizer=opt, class_loss=class_loss, regression_loss=regression_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* save best model\n",
    "import os\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "checkpoint_path = \"models/best_model.h5\"\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "#* early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=25,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(\n",
    "    train, epochs=500, validation_data=val, callbacks=[tensorboard_callback, model_checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = train.as_numpy_iterator()\n",
    "x, y = data_sample.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "img = x[i]\n",
    "pred_coord_img = model.predict(x)[1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt1 = int(pred_coord_img[:2][0] * img.shape[0]), int(pred_coord_img[:2][1] * img.shape[1])\n",
    "pt2 = int(pred_coord_img[2:][0] * img.shape[0]), int(pred_coord_img[2:][1] * img.shape[1])\n",
    "\n",
    "cv2.rectangle(\n",
    "    img= img,\n",
    "    pt1 = pt1,\n",
    "    pt2 = pt2,\n",
    "    color = (0, 255, 0),\n",
    "    thickness=1\n",
    ")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/face_tracker.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models/face_tracker.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = model.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('models/face_tracker.tf')\n",
    "\n",
    "cap = cv2.VideoCapture()\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = model.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
